{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArktlxI5ExVX",
        "outputId": "5a67ef1a-3524-464b-823a-0f8f70b34ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  7 17:36:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO1gLg6mEYeM"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/DIV2K.rar ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycrpqhaV9Dfx"
      },
      "outputs": [],
      "source": [
        "! apt-get install rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbdjm4LmE-qM"
      },
      "outputs": [],
      "source": [
        "!unrar x /content/DIV2K.rar /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTs5ORyHFCOo"
      },
      "outputs": [],
      "source": [
        "%pip install -U transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gLDPEETF1CX"
      },
      "source": [
        "###DIV2K dataset preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eICVzpU7FFvr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from transformers import PoolFormerFeatureExtractor, PoolFormerModel\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerDecodeHead\n",
        "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
        "from datasets import load_dataset, Dataset\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "from typing import Optional, Tuple, Union\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from torchvision.transforms import RandomResizedCrop, Compose, ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzLg4Y7JFvtu"
      },
      "outputs": [],
      "source": [
        "\n",
        "x4dir = ''       #input folder\n",
        "hrdir = ''       #GT folder\n",
        "inputlist = os.listdir(x4dir)\n",
        "random.shuffle(inputlist)\n",
        "trainlist = inputlist[:3550]\n",
        "vallist = inputlist[3550:]\n",
        "\n",
        "my_dict = {'image': [], 'label': []}\n",
        "for ori_img in trainlist:\n",
        "    label = Image.open(hrdir + ori_img.replace('x4',''))\n",
        "    ori_img = Image.open(x4dir + ori_img)\n",
        "    ori_img = ori_img.resize(label.size, resample=0)\n",
        "    my_dict['image'].append(ori_img)\n",
        "    my_dict['label'].append(label)\n",
        "train_dataset = Dataset.from_dict(my_dict)\n",
        "del my_dict\n",
        "\n",
        "my_dict = {'image': [], 'label': []}\n",
        "for ori_img in vallist:\n",
        "    label = Image.open(hrdir + ori_img.replace('x4',''))\n",
        "    ori_img = Image.open(x4dir + ori_img)\n",
        "    ori_img = ori_img.resize(label.size, resample=0)\n",
        "    my_dict['image'].append(ori_img)\n",
        "    my_dict['label'].append(label)\n",
        "val_dataset = Dataset.from_dict(my_dict)\n",
        "del my_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGV8AcdlFxaq"
      },
      "outputs": [],
      "source": [
        "def transforms(examples):\n",
        "    images = [ToTensor()(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    labels = [ToTensor()(label.convert(\"RGB\")) for label in examples[\"label\"]]\n",
        "    cropped = deepcopy([torch.cat([x,y], dim=0) for x, y in zip(images, labels)])\n",
        "    cropped = [RandomResizedCrop(256)(stack) for stack in cropped]\n",
        "    examples[\"pixel_values\"] = [stack[:3] for stack in cropped]\n",
        "    examples[\"annotation\"] = [stack[3:] for stack in cropped]\n",
        "    del examples[\"image\"], examples[\"label\"], images, labels, cropped\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3dndgoSGA_B"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.with_transform(transforms)\n",
        "val_dataset = val_dataset.with_transform(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGcjXPn8GVIK"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS3Ei4RdGT3L"
      },
      "outputs": [],
      "source": [
        "class Img2ImgDecoder(SegformerDecodeHead):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = torch.nn.Conv2d(config.decoder_hidden_size, 3, kernel_size=1)\n",
        "        self.RGBnorm = torch.nn.BatchNorm2d(3)\n",
        "\n",
        "    def forward(self, encoder_hidden_states):\n",
        "        batch_size = encoder_hidden_states[-1].shape[0]\n",
        "        h_4, w_4 = encoder_hidden_states[0].size()[2:]\n",
        "\n",
        "        all_hidden_states = ()\n",
        "        for encoder_hidden_state, mlp in zip(encoder_hidden_states, self.linear_c):\n",
        "            if self.config.reshape_last_stage is False and encoder_hidden_state.ndim == 3:\n",
        "                height = width = int(math.sqrt(encoder_hidden_state.shape[-1]))\n",
        "                encoder_hidden_state = (\n",
        "                    encoder_hidden_state.reshape(batch_size, height, width, -1).permute(0, 3, 1, 2).contiguous()\n",
        "                )\n",
        "\n",
        "            # unify channel dimension\n",
        "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
        "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
        "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
        "            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)\n",
        "            # upsample\n",
        "            encoder_hidden_state = torch.nn.functional.interpolate(\n",
        "                encoder_hidden_state, size=(h_4*4, w_4*4), mode=\"bicubic\", align_corners=False\n",
        "            )\n",
        "            all_hidden_states += (encoder_hidden_state,)\n",
        "\n",
        "        hidden_states = self.linear_fuse(torch.cat(all_hidden_states[::-1], dim=1))\n",
        "        hidden_states = self.batch_norm(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        # logits are of shape (batch_size, RGB, height, width)\n",
        "        logits = self.classifier(hidden_states)\n",
        "        logits = self.RGBnorm(logits)\n",
        "        logits = torch.sigmoid(logits)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hislnatO2Qoj"
      },
      "outputs": [],
      "source": [
        "Encoder = PoolFormerModel.from_pretrained(\"sail/poolformer_s36\"); Encoder.to('cuda:0')\n",
        "Decoder = Img2ImgDecoder.from_pretrained(\"nvidia/mit-b1\"); Decoder.to('cuda:0')\n",
        "#Encoder = PoolFormerModel.from_pretrained(\"/content/drive/MyDrive/Poolformer_Segformer/checkpoint-1200\"); Encoder.to('cuda:0') #\n",
        "#Decoder = Img2ImgDecoder.from_pretrained(\"/content/drive/MyDrive/Poolformer_Segformer/checkpoint-1200\"); Decoder.to('cuda:0') #\n",
        "print('Loading from pretrained weights: Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjr-AS8CGp4M"
      },
      "outputs": [],
      "source": [
        "feature_extractor = PoolFormerFeatureExtractor.from_pretrained(\"sail/poolformer_s36\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFMLt4ytGtC1"
      },
      "outputs": [],
      "source": [
        "class Img2Img(SegformerForSemanticSegmentation):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.segformer = Encoder\n",
        "        self.decode_head = Decoder\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        annotation: Optional[torch.FloatTensor] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SemanticSegmenterOutput]:\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        outputs = self.segformer(\n",
        "            pixel_values,\n",
        "            output_hidden_states=True,  # we need the intermediate hidden states\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[1]\n",
        "\n",
        "        logits = self.decode_head(encoder_hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if annotation is not None:\n",
        "            loss_fct = torch.nn.MSELoss()\n",
        "            loss = loss_fct(logits.squeeze(), annotation.squeeze())\n",
        "\n",
        "        if not return_dict:\n",
        "            if output_hidden_states:\n",
        "                output = (logits,) + outputs[1:]\n",
        "            else:\n",
        "                output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SemanticSegmenterOutput(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvTfnQ-dGvBi"
      },
      "outputs": [],
      "source": [
        "#model = Img2Img.from_pretrained(\"nvidia/mit-b1\")\n",
        "model = Img2Img.from_pretrained(\"/content/drive/MyDrive/Poolformer_Segformer/checkpoint-1200\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epg-PJd7GyTK"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Poolformer_Segformer\",\n",
        "    per_device_train_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=100,\n",
        "    fp16=True,\n",
        "    save_steps=100,\n",
        "    eval_steps=20,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-6,\n",
        "    save_total_limit=10,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30cxxWorG-Ay"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lqko_s2HMT1"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWiEFaLQHP9C"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbvxHQw8n9NR"
      },
      "outputs": [],
      "source": [
        "trainer.state.log_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP8AdOJcyuTk"
      },
      "source": [
        "###Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM2cwoREvR07"
      },
      "outputs": [],
      "source": [
        "import os, tarfile\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def make_targz_one_by_one(output_filename, source_dir):\n",
        "  tar = tarfile.open(output_filename,\"w\")\n",
        "  for root,dir_name,files_list in os.walk(source_dir):\n",
        "    for file in files_list:\n",
        "      pathfile = os.path.join(root, file)\n",
        "      tar.add(pathfile)\n",
        "  tar.close()\n",
        "\n",
        "  files.download(output_filename)\n",
        "\n",
        "\n",
        "make_targz_one_by_one('1224-2-1000-CP_tar', '/content/drive/MyDrive/Poolformer_Segformer/checkpoint-1000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Calxl-OeHUwT"
      },
      "source": [
        "##OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1oRcgF6mHjE"
      },
      "outputs": [],
      "source": [
        "class Img2ImgDecoder(SegformerDecodeHead):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = torch.nn.Conv2d(config.decoder_hidden_size, 3, kernel_size=1)\n",
        "        self.RGBnorm = torch.nn.BatchNorm2d(3)\n",
        "\n",
        "    def forward(self, encoder_hidden_states):\n",
        "        batch_size = encoder_hidden_states[-1].shape[0]\n",
        "        h_4, w_4 = encoder_hidden_states[0].size()[2:]\n",
        "\n",
        "        all_hidden_states = ()\n",
        "        for encoder_hidden_state, mlp in zip(encoder_hidden_states, self.linear_c):\n",
        "            if self.config.reshape_last_stage is False and encoder_hidden_state.ndim == 3:\n",
        "                height = width = int(math.sqrt(encoder_hidden_state.shape[-1]))\n",
        "                encoder_hidden_state = (\n",
        "                    encoder_hidden_state.reshape(batch_size, height, width, -1).permute(0, 3, 1, 2).contiguous()\n",
        "                )\n",
        "\n",
        "            # unify channel dimension\n",
        "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
        "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
        "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
        "            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)\n",
        "            # upsample\n",
        "            encoder_hidden_state = torch.nn.functional.interpolate(\n",
        "                encoder_hidden_state, size=(h_4*4, w_4*4), mode=\"bicubic\", align_corners=False\n",
        "            )\n",
        "            all_hidden_states += (encoder_hidden_state,)\n",
        "\n",
        "        hidden_states = self.linear_fuse(torch.cat(all_hidden_states[::-1], dim=1))\n",
        "        hidden_states = self.batch_norm(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        # logits are of shape (batch_size, RGB, height, width)\n",
        "        logits = self.classifier(hidden_states)\n",
        "        logits = self.RGBnorm(logits)\n",
        "        logits = torch.sigmoid(logits)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhGruK25zk_Q"
      },
      "outputs": [],
      "source": [
        "Encoder = PoolFormerModel.from_pretrained(\"\"); Encoder.to('cuda:0') #checkpoint\n",
        "Decoder = Img2ImgDecoder.from_pretrained(\"\"); Decoder.to('cuda:0') #checkpoint\n",
        "print('Loading from pretrained weights: Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u8MgSmj1A38"
      },
      "outputs": [],
      "source": [
        "feature_extractor = PoolFormerFeatureExtractor.from_pretrained(\"sail/poolformer_s36\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rnih8JN1i6U"
      },
      "outputs": [],
      "source": [
        "class Img2Img(SegformerForSemanticSegmentation):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.segformer = Encoder\n",
        "        self.decode_head = Decoder\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        annotation: Optional[torch.FloatTensor] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SemanticSegmenterOutput]:\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        outputs = self.segformer(\n",
        "            pixel_values,\n",
        "            output_hidden_states=True,  # we need the intermediate hidden states\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[1]\n",
        "\n",
        "        logits = self.decode_head(encoder_hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if annotation is not None:\n",
        "            loss_fct = torch.nn.MSELoss()\n",
        "            loss = loss_fct(logits.squeeze(), annotation.squeeze())\n",
        "\n",
        "        if not return_dict:\n",
        "            if output_hidden_states:\n",
        "                output = (logits,) + outputs[1:]\n",
        "            else:\n",
        "                output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SemanticSegmenterOutput(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKXcvEb1zqv-"
      },
      "outputs": [],
      "source": [
        "model = Img2Img.from_pretrained(\"/content/drive/MyDrive/Poolformer_Segformer/checkpoint-3300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ_jnxn22Uji"
      },
      "source": [
        "#Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4lQcbiWHSlL"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToPILImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNttd7VNHZJ3"
      },
      "outputs": [],
      "source": [
        "image = Image.open('/content/drive/MyDrive/testsets/RNI15/Boy.png')\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBi5d0__HbBh"
      },
      "outputs": [],
      "source": [
        "image = torch.unsqueeze(ToTensor()(image), 0).to('cuda:0')\n",
        "with torch.no_grad(): outputs = model(image)\n",
        "ToPILImage()(outputs['logits'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZdTlW4T7ToS"
      },
      "outputs": [],
      "source": [
        "print(outputs['logits'][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git"
      ],
      "metadata": {
        "id": "pqi0sSueVyye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from thop import profile\n",
        "\n",
        "with torch.cuda.device(0):  # 指定 GPU 編號，如果有的話\n",
        "    model = Img2Img(Encoder.config)\n",
        "    flops, params = profile(model, inputs=(torch.randn(1, 3, 256, 256).to('cuda:0'), torch.randn(1, 3, 256, 256).to('cuda:0')))\n",
        "    print(f\"FLOPs: {flops / 1e9:.2f}G, Parameters: {params / 1e6:.2f}M\")\n"
      ],
      "metadata": {
        "id": "qw8tpou8V7RK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "sP8AdOJcyuTk"
      ],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}